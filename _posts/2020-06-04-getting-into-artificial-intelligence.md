---
title: Getting into artificial intelligence 
author: Dimitris Georgoulas
date: 2020-06-04 11:33:00 +0200
categories: [Career, Experience]
tags: [career, artificial intelligence]
toc: true
---  

# Investigating my intuitions
## An unconventional approach
Motivated by the mysterious nature of human reasoning, during 2019, I found the time to start exploring 
the world of artificial intelligence, 
choosing though a rather unconventional approach. Instead of digging right into machine and deep learning courses, 
and start playing with [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/), I decided to 
start by developing a framework that explains my own intuitions about the brain and work my way from 
there. Once I had the basic principles and a proper structure of such a framework in place, I would compare it 
with the established knowledge to see how close or how far I am. No matter if the outcome would be completely wrong or not, 
I knew that it is always much easier to understand the fundamentals if you have 
first tried to explain them by yourself. During this period and as some things were starting to crystallized, I kind of 
cheated. I started watching talks of the three most influential people of modern AI, 
[Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) 
and [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun). (It would be unfair though not to mention 
[Jeff Hawkins](https://en.wikipedia.org/wiki/Jeff_Hawkins) and [HTM theory](https://www.youtube.com/watch?v=XMB0ri4qgwc&list=PL3yXMgtrZmDqhsFQzwUC9V8MeeVOQ7eZ9) too. 
He is not considered mainstream but I gained a lot of useful insights from his talks too). 
Some things made perfect sense to me, others were completely 
new and some others are yet beyond my comprehension. As I discovered in the process, my thinking led to a model 
that uses [autoencoders](https://www.youtube.com/watch?v=9zKuYvjFFS8) and 
[self supervised learning](https://ai.stackexchange.com/questions/10623/what-is-self-supervised-learning-in-machine-learning). 

> ***Note***: In the rest of this post I try to describe the thought process that led to my initial conclusions.  
>It probably contains inaccuracies or even things that are completely wrong. They are my intuitions investigation, nothing more, 
>so take what you read with a grain of salt. Having said that, I think that some of these concepts are really interesting 
>and need to be tested with actual implementations of well established deep learning models and techniques.

## Concepts correlation (barking to word)
My main intuitions come from observing my own brain and the way I reason about things. 
One very obvious phenomenon that caught my attention and was the starting point of my journey, is what I called _concepts correlation_. 
A typical example of this phenomenon is this: When we hear dog barking, a visual representation of a dog can be 
generated "in our mind". A lot of other things can be generated too but let's focus on the correlation between the 
auditory and visual signals at the moment. The opposite is also true. If we see a muted video of a barking dog, we can 
imagine the barking sound, or in other words, the barking sound is generated by the sight of a barking dog. How this 
correlation between the two signals is achieved? This is what I initially wanted to understand. 
I wanted to understand the mechanism with which **two sensory inputs are correlated with each other, in a 
way that when one of them is activated, it generates a representation of the other**. 

## Representation sets
The way I approached this issue was by thinking in terms of high dimensional sensory input signals that are narrowed down 
to low dimensional output signals which represent higher level concepts (or alternatively called "patterns"). 
This transformation between the input and output space, is achieved 
by multi layer neural networks the input layer of which, contains a larger number of neurons than the output layer. 
Let's not focus for the moment on what drives this transformation and how is achieved, let's focus on its result. A 
low-dimensional, high-level concept, is represented by a set of firing neurons of the output layer. These firing neurons form the _representation set_ 
of this concept. This representation set can be thought of as an encoding of the semantics of the input signal or a compressed 
representation of it. This basic process can be easily depicted with a very simple diagram that also shows the 
notation I use for this type of diagrams.

<img src="/assets/img/sample/ai_notation.PNG" alt="shell_mesh" width="400"/>

In the context of the previous example, when we see a dog, our visual system processes the visual input and outputs 
a signal that represents the pattern of a dog. Let's not focus on what type of neural networks 
are good for image processing nor the exact methods they do it. Let's focus on the fact that the visual field, which is composed of a 
whole bunch of colors and shapes, is processed and narrowed down to a higher level pattern. This pattern is represented 
by a set of firing neurons in the output layer of the neural network, the so called representation set of the dog's pattern. 
The same process takes place in the auditory network too. The barking sound is processed and narrowed down to a low 
dimension concept, represented by its own representation set of firing neurons. But how are these two representation sets 
correlated with each other so that one can be generated by the other? 

## Correlators
Another obvious phenomenon is that in order for two concepts (as previously defined) to be correlated with each other, 
they must be active at the same time. For example when we stand next to a barking dog, the sight of the dog and 
the sound of the barking are processed simultaneously and both representation sets are active at the same time. 
During that time a training process that correlates the two representation sets 
must take place. The training must be done in such a way, that **the representation set generated by a signal of sensory modality A (vision), 
is used to generate an approximation of the representation set of a correlated signal of 
sensory modality B (hearing)**. Approximation in this context means, that this generated representation set should be 
classified to the same class with the representation set of the actual correlated signal of sensory modality B. 
Such a mechanism is described in the following diagram. 

![Desktop View]({{ "/assets/img/sample/correlators2.PNG" | relative_url }})  

There are two sensory modalities involved. Vision and hearing. 
Two representation sets are created, one for each sensor. The two representation sets are the input to a system named _Correlator_. 
One of the two sets has the role of the supervisory signal, in the sense that the other must generate a representation 
set that approximates the supervisory set. The Correlator has two units (neural networks), a _Generator_ and a _Classifier_. 
The generator receives the non supervisory signal and generates a representation set that tries to match the supervisory set. 
The generated set is received by the classifier which also receives the actual supervisory set and outputs a correction or "difference" 
signal. This signal tries to modify the generator so that it generates a set that is classified at the same class 
with the supervisory set. This is the training of the system. After a certain training period the generator will output 
a set that approximates the supervisory set. The generator's output, is also the correlator's output. 

When the supervisory signal 
is the auditory representation set (the auditory concept), what is output from the correlator is a 
_visually generated representation set of an auditory concept_. In the context of the barking dog example, this output 
is the barking "sound" which is generated "in our mind" when we see the muted video of a dog barking. 
The two sensory representation sets must be active together for a sufficient amount of time for the correlator's 
training to take place. There is also a second identical correlator with the difference being that in this one, the supervisory 
signal is different. Eventually, this correlator will receive the actual barking sound and will generate a 
representation of the image of a dog, or more formally it will output an _audibly generated representation set of a 
visual concept_. 

Some notes: 
- The sensory processing units transform a high dimensional input space to a low dimensional output space so the 
input and output signals can also be though of as vectors. 
- The correlation is achieved in the output space, at representation set level, between low-dimensional high-level concepts.
- The output space must be low dimensional enough so that the output vectors are simple enough to facilitate 
the processes that work with them (like the correlation process). They must also be rich enough so that they 
contain the fundamental semantics of the input space and so that they can generate their correlated vectors.
- In the described system, the training of the sensory units is independent of the correlators training. The 
correction signal is only received by the generator. In an alternative scenario, the correction signal produced by the 
classifier, apart from the generator's training might contribute to the sensory units training too. This might be 
necessary so that they produce slightly different representation sets that are easier to correlate. Although, intuitively 
I would say that this is not the case. Imagine that you have seen many dogs and have learned to recognize them, but 
you have never stand next to a barking dog. You have heard barking many times but you have no idea that its a dog's voice. 
At this point the representation sets are well established but not correlated. If you finally see and hear a dog barking 
I think that you would easily correlate the two without much effort. By correlation I mean that you could imagine a dog 
by hearing barking and vice versa. So, I would argue that in this scenario the training is only applied to the correlator 
and not to the sensory units that produce the two representation sets.
- The classifier's correction signal is like a loss function. There might be different types of classifiers.

## Decoders
A correlator's output, is a representation set, or encoding, that lives in the low-dimension output space. This 
encoding can be decoded back to the input space by another unit, the _Decoder_. The decoder receives a representation 
set and generates its input signal. The following diagram is an enriched version of the correlators' diagram which 
contains the decoders too. 

![Desktop View]({{ "/assets/img/sample/decoder1b.png" | relative_url }}) 

In this model, the decoder contains a generator and a classifier similarly to the correlator, so its training is very 
similar too. The high-dimensional signal is the supervisory signal, while the generator tries to generate an approximation of it by 
processing the lower-dimensional signal. The classifier generates a correction signal that trains the generator. 

> ***Note***: When there are two correlated signals then one of them can generate an approximation of the other's 
>representation set. This approximation is very similar to the original set. This means that if it is fed into the 
>decoder, it will create the lower-dimensional signal too. This way the word "dog" can generate a visual representation 
>of a dog.  

<img src="/assets/img/sample/decoder2a.png" alt="shell_mesh" width="400"/>

## Hierarchies
One of the reasons that deep neural networks are good at processing sensory signals (say pictures, which are snapshots
of the real world), is the fact that the input data is _compositional_. A rectangle for example is 
composed of lower level patterns, the lines and the corners. Similarly every object is composed of simpler lower level 
patterns and these patterns from even simpler ones and so on. This compositionality exist not only in visual 
but in all sensory signals. The world data is compositional, so whatever sensor we use to process it will 
receive compositional signals. 

This compositionality can be mirrored to the structure of a hierarchical system that processes compositional data. 
Such a hierarchical system can be structured using the model with correlators and decoders as they are previously defined. 


![Desktop View]({{ "/assets/img/sample/hierarchy_1.png" | relative_url }}) 

## Propagation structures (loops?)
composition-decomposition cycles
Αυτά με την σειρά τους θα μπορούσαν να αρχικοποιούν μια καινούρια συνθετική διαδικασία η οποία καταλήγει να δημιουργεί 
ένα νέο στιγμιότυπο (instance) του αρχικού διανύσματος σχηματίζοντας συνολικά μια διαδικασία διασποράς του σε πολλά
 στιγμιότυπα. Κάθε στιγμιότυπο εδράζεται σε διαφορετικό δίκτυο (υλισμικό/hardware). 

Με αυτόν τον τρόπο διασποράς, ένα νοητικό αντικείμενο εξαπλώνεται σε μια ευρύτερη περιοχή του εγκεφάλου, 
η διάρκεια ζωής του αυξάνεται, αυξάνοντας έτσι την πιθανότητα να συναντηθεί με άλλο νοητικό αντικείμενο σε 
πολλαπλά σημεία με πολλαπλούς τρόπους και να δημιουργήσει πολλαπλούς συσχετισμούς. Καθιστά δηλαδή τον μηχανισμό 
συσχετισμού πιο αποδοτικό, πιο αποτελεσματικό καθώς επίσης και λόγω πλεονασμού (redundancy) ανθεκτικό σε βλάβες. 

Όταν γράφουμε, τα νοητικά αντικείμενα έχουν μεγαλύτερη εξάπλωση και διάρκεια ζωής καθώς τα δημιουργούμε συνεχώς μέχρι 
να ολοκληρώσουμε την συγγραφή της λέξης ή της πρότασης. Ίσως αυτός να είναι ο λόγος για τον οποίο πολλές φορές νιώθουμε
 την έμπνευση να έρχεται καθώς γράφουμε. Διότι δίνουμε στα νοητικά αντικείμενα την ευκαιρία να συσχετιστούν με πολλά 
 άλλα αντικείμενα της ενεργούς κατάστασης, με πολλούς διαφορετικούς τρόπους και να δημιουργήσουν καινούρια, τα οποία 
 με την σειρά τους να συσχετιστούν με άλλα δημιουργώντας νέες αλυσίδες σκέψης. 

## Attention signals
Is the imagined dog the representation set of the dog or the generated visual scene that is generated by the decoder 
using that representation set? I would say the representation set. When we read a book we don’t imagine every single 
word of it but some generic scene that caught our attention. Maybe is not the depth the associated concept of a word 
will be decomposed to that makes the difference but the length of the propagation structure. All concepts are decomposed 
but only some of them create long propagation structures. The others live for a very short period of time. What makes 
some of them to propagate longer? Some attention mechanism. If we somehow insist of thinking of a concept, then we keep 
the propagation structure alive. To keep it alive without explicit sensory input, means that is kept alive only by the 
active state. It can be kept alive indefinitely so this means either that the propagation structure forms a closed loop 
(the last part triggers the first) that needs an external signal (intention signal or attention signal) to be kept 
alive or that our intention generates an input to the structure that regenerates it before it becomes inactive. 
Maybe every propagation loop is capable of staying alive if it receives a signal. If it doesn’t then it fades out. 
How can we model our intention, that insists on thinking in a concept? What is its mechanism that keeps the propagation 
loop alive? We somehow create an activity that keeps the structure alive. The attention signal seems a better idea if 
it can be generic in the sense that the same system creates an identical signal, the attention signal, that will keep 
any propagation loop alive if it is routed to it. 
So there is an attention system that can reach to all others and all others can reach to it. They need to be able to 
reach to it since they need to inform it somehow that they are active so that it can “decide” if it will pay attention 
to them by sending them a signal that will keep them alive. 
This system needs to learn which loops or combination of loops to keep alive at any point in time. This is where an 
internal reward system might be necessary. Whenever we pay our attention to the right things, there is a reward 
(dopamin related?) that increases the current settings of the attention system. What things are the right things 
depends on the situation and the personalized reward system that is different for each person. Every person grows 
up in different environments and what is right every time depends on the environment too. 

## Generalizing the correlation process
Συσχέτιση οποιονδήποτε διανυσμάτων σκέψης
Διερεύνησε την πιθανότητα ο ίδιος αυτός μηχανισμός να μπορεί να χρησιμοποιηθεί για την συσχέτιση οποιονδήποτε 
διανυσμάτων σκέψης, όχι μόνο διανυσμάτων σκέψης τα οποία προέρχονται από αισθητήρια ερεθίσματα. Ένα θέμα είναι 
ότι σε αυτή την περίπτωση δεν είναι ξεκάθαρο το ποιο είναι το εποπτικό διάνυσμα. Στις αισθητήριες αναπαραστάσεις 
είναι το αισθητήριο ερέθισμα. Εδώ ίσως να αρκεί το γεγονός ότι ενεργοποιούνται ταυτόχρονα και εφόσον υπάρχουν 
τουλάχιστον δύο κωδικοποιητές συσχέτισης, στον πρώτο το ένα έχει τον ρόλο του εποπτικού διανύσματος και στον δεύτερο το άλλο. 

## language, correlation of words with concepts. 
words are correlated with other higher concepts.
we don't plan what to say excatly, we only have the higher level concepts. They are input to RNN like and output 
language sequence.  

## active state + sensory inputs fed into RNN equivalent. 


- Unfortunately my resources are limited so I have to put this work on hold for the moment.
- proprioception thoughts 
 

# Some thoughts
## efficiency through quantity
Ένα τέτοιο module μπορεί να μάθει ένα συγκεκριμένο αριθμό πραγμάτων. Μπορεί να υπάρχουν πάρα πολλά πανομοιότυπα με 
αυτό modules καθένα από τα οποία μαθαίνει έναν συγκεκριμένο αριθμό πραγμάτων. Αν όλα τα ακουστικά σήματα έχουν αρχικά 
την δυνατότητα σύνδεσης μέσω κωδικοποιητών συσχέτισης με όλα τα οπτικά και αντιστρόφως τότε το ποια θα μάθουν τι είναι 
τυχαίο. Αυτά τα οποία τυχαίνει να συσχετίζονται ευκολότερα (να δημιουργείται δηλαδή το ένα από το άλλο πιο εύκολα) 
τότε είναι αυτά τα οποία μαθαίνουν το συγκεκριμένο πράγμα και οι συνδέσεις του συγκεκριμένου ακουστικού κυκλώματος με 
τα υπόλοιπα οπτικά κυκλώματα με το οποίο ήταν αρχικά συνδεδεμένο δεν χρησιμοποιούνται και άρα ατονούν.

Το αισθητήριο ερέθισμα τροφοδοτείται όχι μόνο σε ένα δημιουργικό δίκτυο για κάθε μία από τις άλλες αισθήσεις, αλλά 
σε χιλιάδες ίδιου τύπου δίκτυα (λίγων νευρώνων το καθένα) για κάθε αίσθηση. Κάθε ένα από αυτά τα δίκτυα έχει μια 
τυχαία αρχική συνδεσμολογία. Λόγω του ότι ο αριθμός των δικτύων είναι πολύ μεγάλος κάποια από αυτά θα αποδίδουν 
καλύτερα από τα υπόλοιπα σε συγκεκριμένες δοκιμασίες. Για παράδειγμα κάποια από αυτά με λίγες επαναλήψεις 
(λίγα σήματα διόρθωσης) μπορούν να παράγουν ένα διάνυσμα σκέψης το οποίο να ταξινομείται στην ίδια κλάση με 
το διάνυσμα του αισθητήριου ερεθίσματος. Όλα τα υπόλοιπα στον ίδιο αριθμό επαναλήψεων δεν έχουν συγκλίνει 
ακόμα και μπορεί να χρειαζόντουσαν πολλές ακόμα επαναλήψεις. Όταν υπάρχει σύγκλιση από κάποιο ή από κάποια, 
τότε δημιουργείται το αίσθημα της ανταμοιβής και η προσοχή στρέφεται αλλού. Τα υπόλοιπα δίκτυα θα καταλήξουν 
να χρησιμοποιούνται για άλλες δοκιμασίες εκεί που τυχαίνει να είναι καλύτερο το καθένα. Άρα όταν ακούμε τις 
λέξεις γάτα και σκύλος μπορεί να χρησιμοποιείται το ίδιο δίκτυο για να παράξει τις οπτικές αναπαραστάσεις τους 
ενώ ένα άλλο δίκτυο για να παράξει την οπτική αναπαράσταση μιας γλάστρας. 

## Ευρος δυανυσμάτων
A question is how a vector of this abstract representation space is physically represented in the brain. For example 
if a vector represents the presence of a human leg then there is a specific activity pattern that corresponds to that. 
What about the length of the leg? A leg is a leg even if it is short or tall. How the length is represented in the 
brain? Intuitively I would say that a short and a long leg must have a quite similar pattern. So does a black or white 
leg a fat or a thin one etc. So if we focus on the difference between the patterns of two legs that are identical in 
all but the length, we will isolate some specific neurons. These form the area that represents the leg’s length. 

## Αποκωδικοποίηση υψηλών νοητικών αντικείμενων σε αλληλουχίες
Το σύνολο της σκέψης μας αποτελείται από αυτές τις υψηλού επιπέδου βασικές αναπαραστάσεις οι οποίες μπορούν να 
δημιουργήσουν περισσότερες χαμηλότερου επιπέδου, πιο λεπτομερείς αναπαραστάσεις όταν αυτό απαιτείται, για παράδειγμα 
όταν θέλουμε να εκφραστούμε, να επικοινωνήσουμε γλωσσικά ή όταν θέλουμε να ζωγραφίσουμε ένα τοπίο. Όταν μιλάμε, δεν 
σχεδιάζουμε από πριν τι ακριβώς θέλουμε να πούμε και πώς. Ξέρουμε τα βασικά νοητικά αντικείμενα τα οποία θέλουμε να 
επικοινωνήσουμε και από αυτά δημιουργούμε μια αλληλουχία λέξεων. Τα βασικά νοητικά αντικείμενα είναι δηλαδή μη 
γλωσσικά και είναι οι υψηλού επιπέδου αναπαραστάσεις οι οποίες μπορούν να δημιουργήσουν χαμηλότερου επιπέδου 
λεπτομερείς αναπαραστάσεις διαφόρων συστημάτων όχι μόνο ομιλίας. Μπορούν για παράδειγμα να δημιουργήσουν τόσο 
γλωσσική αλληλουχία όσο και κινητική αλληλουχία (όταν θέλουμε για παράδειγμα να περιγράψουμε ένα τοπίο ζωγραφίζοντάς 
το ή όταν εκτελούμε μια συγκεκριμένη κινητική εργασία).

# Some thoughts about the future of AI 
I was always fascinated 
by the mysteries of the human brain, the mechanisms of reasoning and the origins of consciousness. I firmly believe that 
there is a high chance that these mysteries will be unlocked within our life time, say the next forty years, 
leading to the birth of human level generic artificial intelligence, in what will be mankind's most important 
and influential achievement and a turning point to the entire history of civilization. No matter how groundbreaking this 
might seem, it would probably be just the beginning. 
If the fundamental mechanisms of intelligence are understood, then there is probably no reason to think that the 
artificial version of it must be limited to human level. There are some mind blowing paths from there, leading to 
super intelligences with god like powers or the blending of physical and artificial intelligence that will give rise to 
updated versions of humans. These things are just science fiction of course, but the fact is that the field of artificial 
intelligence is hot again, and no one knows where it will eventually lead. It is also a fact that it is already part of 
our everyday lives and will increasingly continue to do so in the future.    

## why NNs?
world has compositionality
